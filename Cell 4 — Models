class LowRankLinear(nn.Module):
    def __init__(self, in_features, out_features, rank, bias=True):
        super().__init__()
        self.U = nn.Parameter(torch.randn(in_features, rank)*0.01)
        self.V = nn.Parameter(torch.randn(rank, out_features)*0.01)
        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None
    def forward(self, x):
        x = x @ self.U @ self.V
        if self.bias is not None: x += self.bias
        return x

class LowRankAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dims=[128,64], latent_dim=16, rank_factor=8, dropout=0.2):
        super().__init__()
        rank = max(1, min(hidden_dims[0], input_dim)//rank_factor)
        self.encoder = nn.Sequential(
            LowRankLinear(input_dim, hidden_dims[0], rank), nn.BatchNorm1d(hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout),
            LowRankLinear(hidden_dims[0], hidden_dims[1], rank), nn.BatchNorm1d(hidden_dims[1]), nn.ReLU(), nn.Dropout(dropout),
            LowRankLinear(hidden_dims[1], latent_dim, rank)
        )
        self.decoder = nn.Sequential(
            LowRankLinear(latent_dim, hidden_dims[1], rank), nn.BatchNorm1d(hidden_dims[1]), nn.ReLU(), nn.Dropout(dropout),
            LowRankLinear(hidden_dims[1], hidden_dims[0], rank), nn.BatchNorm1d(hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout),
            LowRankLinear(hidden_dims[0], input_dim, rank)
        )
    def forward(self, x): z = self.encoder(x); return self.decoder(z), z
    def compute_anomaly_score(self, x):
        with torch.no_grad():
            x_hat,_ = self.forward(x)
            score = F.mse_loss(x_hat, x, reduction='none').mean(1)
            score[torch.isnan(score)] = 0
            return score
    def count_params(self): return sum(p.numel() for p in self.parameters() if p.requires_grad)

class StandardAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dims=[128,64], latent_dim=16, dropout=0.2):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]), nn.BatchNorm1d(hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dims[0], hidden_dims[1]), nn.BatchNorm1d(hidden_dims[1]), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dims[1], latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dims[1]), nn.BatchNorm1d(hidden_dims[1]), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dims[1], hidden_dims[0]), nn.BatchNorm1d(hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dims[0], input_dim)
        )
    def forward(self, x): z = self.encoder(x); return self.decoder(z), z
    def compute_anomaly_score(self, x):
        with torch.no_grad():
            x_hat,_ = self.forward(x)
            score = F.mse_loss(x_hat, x, reduction='none').mean(1)
            score[torch.isnan(score)] = 0
            return score
    def count_params(self): return sum(p.numel() for p in self.parameters() if p.requires_grad)
